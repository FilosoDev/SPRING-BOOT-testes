<?xml version="1.0" encoding="UTF-8"?>
<?xml-stylesheet type="text/xsl" media="screen" href="/~d/styles/atom10full.xsl"?><?xml-stylesheet type="text/css" media="screen" href="http://feeds.feedburner.com/~d/styles/itemcontent.css"?><feed xmlns="http://www.w3.org/2005/Atom" xmlns:dc="http://purl.org/dc/elements/1.1/"><title>JBoss Tools Aggregated Feed</title><link rel="alternate" href="http://tools.jboss.org" /><subtitle>JBoss Tools Aggregated Feed</subtitle><dc:creator>JBoss Tools</dc:creator><atom10:link xmlns:atom10="http://www.w3.org/2005/Atom" rel="self" type="application/atom+xml" href="http://feeds.feedburner.com/jbossbuzz" /><feedburner:info xmlns:feedburner="http://rssnamespace.org/feedburner/ext/1.0" uri="jbossbuzz" /><atom10:link xmlns:atom10="http://www.w3.org/2005/Atom" rel="hub" href="http://pubsubhubbub.appspot.com/" /><entry><title>A faster way to access JDK Flight Recorder data</title><link rel="alternate" href="https://developers.redhat.com/articles/2021/11/23/faster-way-access-jdk-flight-recorder-data" /><author><name>Andrew Azores</name></author><id>ef84d7f6-afb6-42b4-872d-e14f92c71fc2</id><updated>2021-11-23T07:00:00Z</updated><published>2021-11-23T07:00:00Z</published><summary type="html">&lt;p&gt;This article introduces a special rule definition in &lt;a href="https://developers.redhat.com/articles/2021/10/18/announcing-cryostat-20-jdk-flight-recorder-containers"&gt;Cryostat 2.0&lt;/a&gt; that lets you access &lt;a href="https://developers.redhat.com/blog/2020/08/25/get-started-with-jdk-flight-recorder-in-openjdk-8u"&gt;JDK Flight Recorder&lt;/a&gt; (JFR) data on the fly, without waiting for your application's normally scheduled archive process. We'll introduce Cryostat's new &lt;code&gt;POST&lt;/code&gt; rule definition and show you how to use it to quickly diagnose performance problems in &lt;a href="https://developers.redhat.com/topics/containers"&gt;containerized applications&lt;/a&gt; running on &lt;a href="https://developers.redhat.com/topics/kubernetes"&gt;Kubernetes&lt;/a&gt; and &lt;a href="https://developers.redhat.com/openshift"&gt;Red Hat OpenShift&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Read the series&lt;/strong&gt;:&lt;/p&gt; &lt;p&gt;We're publishing a series of hands-on guides to using Cryostat 2.0, which is JDK Flight Recorder (JFR) for Kubernetes. Read all of the articles in this series:&lt;/p&gt; &lt;ul&gt;&lt;li&gt;Part 1: &lt;a href="https://developers.redhat.com/articles/2021/10/18/announcing-cryostat-20-jdk-flight-recorder-containers"&gt;Get started with Cryostat 2.0&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Part 2: &lt;a href="https://developers.redhat.com/articles/2021/10/26/configuring-java-applications-use-cryostat"&gt;Configuring Java applications to use Cryostat&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Part 3: &lt;a href="https://developers.redhat.com/articles/2021/11/02/java-monitoring-custom-targets-cryostat"&gt;Java monitoring for custom targets with Cryostat&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Part 4: &lt;a href="https://developers.redhat.com/articles/2021/11/09/automating-jdk-flight-recorder-containers"&gt;Automating JDK Flight Recorder in containers&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Part 5: &lt;a href="https://developers.redhat.com/articles/2021/11/16/custom-jfr-event-templates-cryostat-20"&gt;Creating Custom JFR event templates with Cryostat 2.0&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt;&lt;p class="Indent1"&gt;&lt;strong&gt;Note&lt;/strong&gt;: &lt;a href="https://access.redhat.com/documentation/en-us/openjdk/11/html/release_notes_for_cryostat_2.0"&gt;The Red Hat build of Cryostat 2.0&lt;/a&gt; is now widely available in technology preview. The Red Hat build includes the &lt;a href="https://catalog.redhat.com/software/operators/detail/60ee049a744684587e218ef5"&gt;Cryostat Operator&lt;/a&gt; to simplify and automate Cryostat deployment on OpenShift.&lt;/p&gt; &lt;h2&gt;Continuous monitoring for service disruptions&lt;/h2&gt; &lt;p&gt;Consider a scenario where a cluster administrator has deployed Cryostat in a project namespace and enabled continuous application monitoring using &lt;a href="https://developers.redhat.com/articles/2021/11/09/automating-jdk-flight-recorder-containers"&gt;Cryostat's automated rules&lt;/a&gt;. Such a rule definition might look like this:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-json"&gt; { "name": "continuousMonitoring", "description": "Enable the Continuous template on all parts of our service", "matchExpression": "target.labels.platform[‘app’]==’myService’", "eventSpecifier": "template=Continuous,type=TARGET", "archivalPeriodSeconds": 3600, "preservedArchives": 4 } &lt;/code&gt; &lt;/pre&gt; &lt;p&gt;This rule, called &lt;code&gt;continuousMonitoring&lt;/code&gt;, creates a recording using the &lt;code&gt;Continuous&lt;/code&gt; event template on all targets with the &lt;code&gt;app=myService&lt;/code&gt; label. It then copies the recording data to archives once every hour and maintains four of these archived copies.&lt;/p&gt; &lt;p&gt;In this scenario, the project contains many instances of &lt;a href="https://developers.redhat.com/topics/microservices"&gt;microservices&lt;/a&gt; that together provide a larger external service to customers. The microservices make internal requests to each other while servicing customer traffic. Occasionally, for reasons the cluster administrator doesn't yet understand, a hiccup occurs, and the time to service a customer’s request suddenly spikes.&lt;/p&gt; &lt;h2&gt;Using the POST rule for OpenJDK flight data&lt;/h2&gt; &lt;p&gt;Let say an admin receives notification that a spike has just occurred, but the &lt;code&gt;continuousMonitoring&lt;/code&gt; rule is not set to copy the JDK Flight Recorder data into archives for another 40 minutes. Obviously, the admin doesn't want to wait that long: They need to discover what caused the service latency spike and fix it right away. For that, they need an immediate way to capture the JDK Flight Recorder data.&lt;/p&gt; &lt;p&gt;Cryostat 2.0 lets cluster admins capture OpenJDK flight data immediately by &lt;code&gt;POST&lt;/code&gt;ing a rule definition with the following form:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-json"&gt; { "matchExpression": "target.annotations.platform[‘app’]==’myService’", "eventSpecifier": "archive" } &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Notice the &lt;code&gt;eventSpecifier: archive&lt;/code&gt; property. This is a special case that instructs Cryostat that, rather than creating a new recording in all of the matching targets, it should immediately copy recording data from all of the matching targets to the Cryostat archives. Using the same &lt;code&gt;matchExpression&lt;/code&gt; as the original rule definition ensures that the archival action applies to the same targets.&lt;/p&gt; &lt;p&gt;As an example, if the admin suspected that the problem lived in a particular subset of the microservice targets, they could fine-tune the &lt;code&gt;matchExpression&lt;/code&gt; to suit. For example, they could focus their search on the login service:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-json"&gt; { "matchExpression": "target.annotations.platform[‘app’]==’myService’ &amp;&amp; /^my-app-login-service-/.test(target.alias)", "eventSpecifier": "archive" } &lt;/code&gt; &lt;/pre&gt; &lt;p&gt;After &lt;code&gt;POST&lt;/code&gt;ing the new rule definition to Cryostat, the admin will immediately have access to a copy of each microservice replica’s recording data in the Cryostat archives. Let’s take a look:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt; $ CRYOSTAT=https://cryostat.example.com $ curl -F name="continuousMonitoring" -F description=”Enable the Continuous template on all parts of our service” -F matchExpression=”target.annotations.platform[‘app’]==’myService’” -F eventSpecifier=”template=Continuous,type=TARGET” -F archivalPeriodSeconds=3600 -F preservedArchives=4 $CRYOSTAT/api/v2/rules $ # some time passes $ curl -F matchExpression=”target.annotations.platform[‘app’]==’myService’ &amp;&amp; /^my-app-login-service-/.test(target.alias)” -F eventSpecifier=archive $CRYOSTAT/api/v2/rules $ curl $CRYOSTAT/api/v1/recordings &lt;/code&gt; &lt;/pre&gt; &lt;p&gt;From here, they can process the JSON response and take various actions. The response is an array of objects containing information about each recording in the archives. The recording names can be filtered to isolate the ones produced by our &lt;code&gt;archive&lt;/code&gt; rule. The admin can then use the JSON objects’ &lt;code&gt;reportUrl&lt;/code&gt; or &lt;code&gt;downloadUrl&lt;/code&gt; properties to get an HTML document containing either an automated analysis or the full JDK Flight Recorder data dump.&lt;/p&gt; &lt;h2&gt;Conclusion&lt;/h2&gt; &lt;p&gt;Regularly archived JDK Flight Recorder data recordings are adequate for diagnostics purposes most of the time. But sometimes you need more immediate access to data. In this article, you've learned how to use Cryostat's new &lt;code&gt;POST&lt;/code&gt; rule format to access JDK Flight Recorder data on the fly. As a cluster admin, you can respond to system disruptions by &lt;code&gt;POST&lt;/code&gt;ing a specially formulated rule, then batch retrieve the data to diagnose disruptions in your containerized application's performance. Visit &lt;a href="http://cryostat.io/"&gt;Cryostat.io&lt;/a&gt; for more about this and other new features in Cryostat 2.0.&lt;/p&gt; The post &lt;a href="https://developers.redhat.com/articles/2021/11/23/faster-way-access-jdk-flight-recorder-data" title="A faster way to access JDK Flight Recorder data"&gt;A faster way to access JDK Flight Recorder data&lt;/a&gt; appeared first on &lt;a href="https://developers.redhat.com/blog" title="Red Hat Developer"&gt;Red Hat Developer&lt;/a&gt;. &lt;br /&gt;&lt;br /&gt;</summary><dc:creator>Andrew Azores</dc:creator><dc:date>2021-11-23T07:00:00Z</dc:date></entry><entry><title>SmallRye Stork meets Quarkus</title><link rel="alternate" href="&#xA;                https://quarkus.io/blog/smallrye-stork-intro/&#xA;            " /><author><name>Michał Szynkiewicz (https://twitter.com/mszynkiewicz)</name></author><id>https://quarkus.io/blog/smallrye-stork-intro/</id><updated>2021-11-23T00:00:00Z</updated><published>2021-11-23T00:00:00Z</published><summary type="html">Last week, the Quarkus Insights spotlighted a new project called SmallRye Stork and its integration in Quarkus. In this blog post, we’re going to give you a concise introduction to Stork. What’s the problem? Nowadays, systems we build consist of numerous applications, often called microservices. The system’s overall behavior emerges...</summary><dc:creator>Michał Szynkiewicz (https://twitter.com/mszynkiewicz)</dc:creator><dc:date>2021-11-23T00:00:00Z</dc:date></entry><entry><title>Bring your Kubernetes workloads to the edge</title><link rel="alternate" href="https://developers.redhat.com/articles/2021/11/22/bring-your-kubernetes-workloads-edge" /><author><name>Dejan Bosanac</name></author><id>71bb95fc-35c0-4d77-852f-cc196288a397</id><updated>2021-11-22T07:00:00Z</updated><published>2021-11-22T07:00:00Z</published><summary type="html">&lt;p&gt;Although cloud-based applications continue to grow, some use cases require moving workloads out of cloud data centers. The reason is usually to keep your computing power closer to the users, the source of data, or other things you want to control. Instead of running these workloads as separate entities, you might want to create uniform systems, extending clouds to the edge. This technique is known as &lt;a href="https://developers.redhat.com/topics/edge-computing"&gt;edge computing&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;The past few years have seen a proliferation of edge computing infrastructure. Today you have a wealth of options, from running &lt;a href="https://developers.redhat.com/topics/containers"&gt;containers&lt;/a&gt; directly in container runtimes (such as &lt;a href="https://podman.io/"&gt;Podman&lt;/a&gt;), to joining nodes to &lt;a href="https://developers.redhat.com/topics/kubernetes"&gt;Kubernetes&lt;/a&gt; clusters, to running whole lightweight Kubernetes distributions on edge nodes.&lt;/p&gt; &lt;p&gt;As infrastructure becomes widely accessible, developers need to think through the edge computing journey. An important question in this arena is, How do we build our workloads for this new world? This article discusses the current state of tools for managing containers at the edge, including what &lt;a href="https://webassembly.org/"&gt;WebAssembly&lt;/a&gt; (also known as Wasm) offers in this domain and what to expect from the field of edge computing in the near future.&lt;/p&gt; &lt;h2&gt;Managing edge workloads in Kubernetes&lt;/h2&gt; &lt;p&gt;Before we dive into technical details, let's quickly recap what's special about edge workloads. This is the foundation for further technical discussions and solutions.&lt;/p&gt; &lt;p&gt;If we start with the premise that the edge is an extension of the cloud, not independent on-site infrastructure, we can better understand the emphasis on using existing cloud-native tools to develop, deploy, and manage edge workloads. Ideally, we want to create a continuum between devices, edge, and cloud, and treat them as a single system. We want our experience to be as similar as possible when developing the different parts of that system.&lt;/p&gt; &lt;p&gt;But workloads running in the cloud and on the edge encounter different environments. We need to account for those differences in order to build successful systems. Here are some of the most commonly recognized traits of edge workloads:&lt;/p&gt; &lt;ul&gt;&lt;li&gt;&lt;strong&gt;Mixed architectures&lt;/strong&gt;: While running in the cloud assumes a uniform hardware architecture among nodes running the workload, you can expect more variety at the edge. A common example is to target ARM platforms, such as RaspberryPi, in production while developing your workloads on x86 Linux or Mac OS X workstations.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Access to peripherals&lt;/strong&gt;: Workloads running in the cloud rarely need access to specialized hardware on the nodes, with the exception of using GPUs for &lt;a href="https://developers.redhat.com/topics/ai-ml"&gt;machine learning&lt;/a&gt;. In contrast, one of the main points of edge workloads is to interact with their environments. Although it's not always the case, you commonly want access to sensors, actuators, cameras, and other devices connecting over low-range protocols such as Bluetooth. You need to take additional care to support this kind of access to your services.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Resource consumption&lt;/strong&gt;: While the whole premise of the cloud is painless horizontal resource scaling, that is not the case on the edge. Even the best scenarios offer very limited clusters that you can't easily autoscale. So, you need to pay extra attention to the compute resources available to your edge workload. Things such as CPU and memory usage, network availability, and bandwidth play very important roles when choosing technologies and practices for edge workloads.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Size&lt;/strong&gt;: This trait also fits under resource consumption, but it's important enough to stand on its own. Because network bandwidth and storage on the nodes can be limited, you must account for the distribution size of edge workloads.&lt;/li&gt; &lt;/ul&gt;&lt;h2&gt;Containers at the edge&lt;/h2&gt; &lt;p&gt;Having all of this in mind, let's review the current state of running containers at the edge.&lt;/p&gt; &lt;h3&gt;How to build images for different targets&lt;/h3&gt; &lt;p&gt;We will start with building images for a targeted architecture. There are multiple ways to do builds, the most obvious is to use the actual hardware or virtual machines of the targeted architecture to build the images. Although this works, it could be slow and impractical to fully automate in the actual build process.&lt;/p&gt; &lt;p&gt;Luckily, most of the image-building tools we use today support a target architecture that's different from the system doing the build. The following example shows both &lt;a href="https://docs.docker.com/buildx/working-with-buildx/"&gt;Docker buildx&lt;/a&gt; and &lt;a href="https://buildah.io/"&gt;Buildah&lt;/a&gt; building images for different targets:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ docker buildx build --platform linux/arm -t quay.io/dejanb/drogue-dht-py -f Dockerfile --push . $ buildah bud --tag quay.io/dejanb/drogue-dht-py --override-arch arm64 Dockerfile&lt;/code&gt;&lt;/pre&gt; &lt;h3&gt;How to run containers at the edge&lt;/h3&gt; &lt;p&gt;Now that we have our images, let's see how to run them. Again, our two usual suspects, Podman and Docker, provide all we need to enable our workload to access the peripherals it needs. Consider the following example:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ podman run --privileged --rm -ti --device=/dev/gpiochip0 \ -e ENDPOINT=https://http.sandbox.drogue.cloud/v1/foo \ -e APP_ID=dejanb \ -e DEVICE_ID=pi \ -e DEVICE_PASSWORD=foobar \ -e GEOLOCATION="{\"lat\": \"44.8166\", \"lon\": \"20.4721\"}" \ quay.io/dejanb/drogue-dht-py:latest&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Two important actions in this command are worth highlighting: To access a peripheral device, you need to run the container in a "privileged" mode and pass the path to the device you want to access. You can find the whole example used in this discussion in my &lt;a href="https://github.com/dejanb/drogue-dht-py"&gt;GitHub repository&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;Podman, used in this example, has more tricks for allowing you to manage your workloads on the edge. First, this very simple example runs only a single container, but Podman allows you to pack multiple containers into a single pod. This is very useful on its own but also makes it easier to deploy those pods on Kubernetes clusters if needed.&lt;/p&gt; &lt;p&gt;Additionally, Podman's excellent &lt;a href="https://www.redhat.com/sysadmin/podman-shareable-systemd-services"&gt;integration with systemd&lt;/a&gt; and ability to &lt;a href="https://www.redhat.com/sysadmin/podman-auto-updates-rollbacks"&gt;auto-update (and rollback) containers&lt;/a&gt; makes it an ideal lightweight platform for running containers at the edge. For more complex use cases, you could integrate the application into existing or new Kubernetes clusters. But there's nothing stopping you from starting simply with Podman and moving to more complex scenarios as you need them.&lt;/p&gt; &lt;h2&gt;WebAssembly and Wasi&lt;/h2&gt; &lt;p&gt;Until recently, the &lt;a href="https://opencontainers.org/"&gt;Open Container Initiative (OCI)&lt;/a&gt; was the only game in town when it came to running containers in the cloud. Naturally, that limitation carried over to the edge, as well. Lately, however, more attention is being paid to &lt;a href="https://webassembly.org/"&gt;WebAssembly&lt;/a&gt; (or Wasm) as an alternative approach.&lt;/p&gt; &lt;p&gt;So, what is WebAssembly? It's an open standard for defining small and portable virtual machines. It was designed for embedding high-performance native applications into web pages. But since then, it has found its way outside of web browsers. For example, &lt;a href="https://wasi.dev/"&gt;Wasi&lt;/a&gt; provides a system interface that allows you to run WebAssembly binaries on servers.&lt;/p&gt; &lt;p&gt;The rest of this article discusses the current state of running Wasm payloads in the cloud and how the format can play a role on the edge in the future.&lt;/p&gt; &lt;h2&gt;WebAssembly in the cloud (with Rust)&lt;/h2&gt; &lt;p&gt;Let's see how we can create WebAssembly workloads and run them in the cloud. For this example, we'll use a simple &lt;a href="https://www.rust-lang.org/"&gt;Rust&lt;/a&gt; program that sends and receives &lt;a href="https://cloudevents.io/"&gt;CloudEvents&lt;/a&gt; using HTTP.&lt;/p&gt; &lt;p&gt;Although this particular example uses Rust, most of today's popular programming languages can be compiled to WebAssembly. Even so, Rust is interesting in this environment because it provides a modern language that is memory safe and can match C/C++ in terms of performance and binary size. Both are important traits in edge computing workloads.&lt;/p&gt; &lt;p&gt;Lucky for us, Rust comes with great out-of-the-box support for compiling programs for multiple target architectures. In that sense, Wasm is just another target. So, executing the following command adds a new target to the system:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ rustup target add wasm32-wasi &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Then, with this next command, you can easily build a Wasm binary:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ cargo build --target wasm32-wasi --release &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;If you check the binary, you can see that its size is just around 3MB, which is a great start.&lt;/p&gt; &lt;p&gt;Now we just need an environment in which to run this binary. Because we are planning to run it on the server side, Wasi is the main candidate. &lt;a href="https://wasmtime.dev/"&gt;Wasmtime&lt;/a&gt; is an appropriate runtime for the job because it supports Wasi binaries.&lt;/p&gt; &lt;h3&gt;Running Kubernetes nodes on WebAssembly&lt;/h3&gt; &lt;p&gt;So, how can we bring all this to the cloud? &lt;a href="https://krustlet.dev/"&gt;Krustlet&lt;/a&gt; is a project that implements &lt;a href="https://kubernetes.io/docs/reference/command-line-tools-reference/kubelet/"&gt;Kubelets&lt;/a&gt; (Kubernetes nodes) that can run Wasm and Wasi payloads. Krustlet actually uses Wasmtime to run the binaries. But before we can get those binaries to the Krustlet, we need to package them as OCI container images and store them in a container registry. The &lt;a href="https://github.com/engineerd/wasm-to-oci"&gt;wasm-to-oci&lt;/a&gt; project let us do that:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ wasm-to-oci push target/wasm32-wasi/release/ce-wasi-example.wasm ghcr.io/dejanb/ce-wasi-example:latest &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The important thing to note here is that our OCI image is small—very similar in size to the binary on which it is based—which is one criterion I mentioned as important for edge use cases. The other thing to note is that not all container registries accept this kind of image today, but that should change soon.&lt;/p&gt; &lt;h3&gt;Create a pod and schedule it&lt;/h3&gt; &lt;p&gt;With a container image ready, you can create a pod and schedule it in your cluster:&lt;/p&gt; &lt;pre&gt; &lt;code&gt;apiVersion: v1 kind: Pod metadata: name: ce-wasi-example labels: app: ce-wasi-example annotations: alpha.wasi.krustlet.dev/allowed-domains: '["https://postman-echo.com"]' alpha.wasi.krustlet.dev/max-concurrent-requests: "42" spec: automountServiceAccountToken: false containers: - image: ghcr.io/dejanb/ce-wasi-example:latest imagePullPolicy: Always name: ce-wasi-example env: - name: RUST_LOG value: info - name: RUST_BACKTRACE value: "1" - name: ECHO_SERVICE_URL value: "https://postman-echo.com/post" tolerations: - key: "node.kubernetes.io/network-unavailable" operator: "Exists" effect: "NoSchedule" - key: "kubernetes.io/arch" operator: "Equal" value: "wasm32-wasi" effect: "NoExecute" - key: "kubernetes.io/arch" operator: "Equal" value: "wasm32-wasi" effect: "NoSchedule"&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The special parts in this configuration are defining tolerations for the pod, marking a workload as &lt;code&gt;wasm32-wasi&lt;/code&gt; architecture, and telling the cluster to schedule the container on the Krustlet node.&lt;/p&gt; &lt;p&gt;And that's it: We've gone from Wasm to the cloud in a few simple steps. You can find the whole demo covered in this section in my &lt;a href="https://github.com/dejanb/ce-wasi-example"&gt;GitHub repository&lt;/a&gt;.&lt;/p&gt; &lt;h2&gt;Is WebAssembly ready to use?&lt;/h2&gt; &lt;p&gt;You might be wondering why we would want to run WebAssembly in the cloud. How does it compare to containers? And is it useful in the context of edge computing? Let's consider these questions next.&lt;/p&gt; &lt;h3&gt;WebAssembly versus containers&lt;/h3&gt; &lt;p&gt;We'll start with how WebAssembly improves on OCI containers. Wasm was designed to be high-performing and portable, so there should be no surprise that it really shines in those areas. Remember that the OCI container that we created from our demo binary was about 3MB large? That's a huge difference compared to container images, which are usually hundreds of megabytes in size. There are many ways to minimize the size of your container images, but in any case, they will probably be an order of magnitude bigger than comparable Wasm binaries (and the corresponding images).&lt;/p&gt; &lt;p&gt;With its compact size, great runtime performance, and startup speeds, it's no wonder that Wasm was employed in a lot of applications as embedded logic for existing systems, such as creating filters, plugins, and so on. But here we are trying to figure how to build whole applications using WebAssembly. This brings us back to Wasi and the state of its readiness to replace containers.&lt;/p&gt; &lt;h3&gt;Wasi is in early development&lt;/h3&gt; &lt;p&gt;Containers rely on the underlying kernel to enable the sandboxing and security of the process. In contrast, Wasi was designed with its own system interface for that protection. This means that the layer between your services and operating system needs to be implemented with all these new interface specifications of Wasi. So, runtimes such as Wasmtime will gradually catch up with containers.&lt;/p&gt; &lt;p&gt;If we go back to our example, you might notice that we haven't been ambitious and tried something complicated such as reading sensors. We merely used HTTP calls and made it work nicely with the &lt;a href="https://github.com/cloudevents/sdk-rust"&gt;Rust CloudEvents SDK&lt;/a&gt;. Moreover, the connection was accomplished using an &lt;a href="https://github.com/deislabs/wasi-experimental-http"&gt;experimental HTTP library&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;This lag in development does not mean that Wasi lacks potential, just that it's in early development. The framework still needs proper support for networking, threading, device interfaces, and such to be really ready for most common edge use cases.&lt;/p&gt; &lt;p&gt;Finally, you can take virtually any code you have and run it as a container. Wasi, even when fully developed, will probably have some restrictions on the libraries and features used. How all that will look remains to be seen and will probably differ between ecosystems.&lt;/p&gt; &lt;h2&gt;Conclusion&lt;/h2&gt; &lt;p&gt;This article started by discussing special considerations for edge workloads and how container environments deal with some of them. We also saw what WebAssembly brings to the table. Although WebAssembly is quite new, with the amount of buzz around it these days I expect we'll see very interesting developments in the near future. This area will be good to watch and contribute to, and the developer experience for edge computing will keep improving. I hope that I managed to spark your interest in this topic and that you will find more &lt;a href="https://developers.redhat.com/topics/edge-computing"&gt;topics in edge computing&lt;/a&gt; to explore.&lt;/p&gt; The post &lt;a href="https://developers.redhat.com/articles/2021/11/22/bring-your-kubernetes-workloads-edge" title="Bring your Kubernetes workloads to the edge"&gt;Bring your Kubernetes workloads to the edge&lt;/a&gt; appeared first on &lt;a href="https://developers.redhat.com/blog" title="Red Hat Developer"&gt;Red Hat Developer&lt;/a&gt;. &lt;br /&gt;&lt;br /&gt;</summary><dc:creator>Dejan Bosanac</dc:creator><dc:date>2021-11-22T07:00:00Z</dc:date></entry><entry><title type="html">Edge medical diagnosis - Common architectural elements</title><link rel="alternate" href="http://www.schabell.org/2021/11/edge-medical-diagnosis-common-architectural-elements.html" /><author><name>Eric D. Schabell</name></author><id>http://www.schabell.org/2021/11/edge-medical-diagnosis-common-architectural-elements.html</id><updated>2021-11-22T06:00:00Z</updated><content type="html">Part 2 - Common architectural elements In our  from this series we introduced a use case around edge medical diagnosis for the healthcare industry. The process was laid out how we approached the use case and how portfolio solutions are the base for researching a generic architecture.  The only thing left to cover was the order in which you'll be led through the details. This article starts the real journey at the very top, with a generic architecture from which we'll discuss the common architectural elements one by one. ARCHITECTURE REVIEW As mentioned before, the architectural details covered here are based on real solutions using open source technologies. The example scenario presented here is a generic common architecture that was uncovered while researching those solutions. It's our intent to provide guidance and not deep technical details. This section covers the visual representations as presented, but it's expected that they'll be evolving based on future research. There are many ways to represent each element in this architecture, but we've chosen a format that we hope makes it easy to absorb. Feel free to post comments at the bottom of this post, or  with your feedback. Now let's take a look at the details in this architecture and outline the solution. FROM SPECIFIC TO GENERIC Before diving into the common elements, it might be nice to understand that this is not a catch all for every possible solution. It's a collection of identified elements that we've uncovered in multiple customer implementations. These elements presented here are then the generic common architectural elements that we've identified and collected into the generic architecture.  It's our intent to provide an architecture for guidance and not deep technical details. You're smart enough to figure out wiring integration points in your own architectures. You're capable of slotting in the technologies and components you've committed to in the past where applicable.  It's our job here to describe the architectural generic components and outline a few specific cases with visual diagrams so that you're able to make the right decisions from the start of your own projects. Another challenge has been how to visually represent the architecture. There are many ways to represent each element, but we've chosen some icons, text and colours that we hope are going to make it all easy to absorb. Now let's take a quick tour of the generic architecture and outline the common elements uncovered in our research. DIAGNOSTIC FACILITY - EDGE SERVICES Starting on the left side of the diagram, which is by no means a geographical necessity, there are two elements that represent external systems that are integrated with the core elements of this architecture.  The first are edge devices, covering basically everything that is used in the field from clinical personnel, patients, to partnering vendors. These can be anything from sensor devices to mobile units such as phones or tablets, but certainly not limited to just these. It's a grouping to identify functionality on the edge of this use case. The second is called x-ray diagnostic server, a slightly specific element containing all of the remote devices, such as an x-ray machine, capturing diagnostic information such as patient images. DIAGNOSTIC FACILITY - CONTAINER PLATFORM These elements in the common architecture are found on location at the diagnostic facilities and can be deployed on a container platform to indicate the cloud-ready nature of this architecture.  The image upload application, is used to take the edge captured images and push them back to the medical center for analysis using complexer solutions than are deployed on site. These can also then be used to improve the various artificial intelligence and machine learning models applied to the detection application. They also provide historical data sets of actual images, that after being anonymised, can further many of the research projects such organisations pursue. Pneumonia detection application is used in the diagnostic facility on site to analyse the images of patients and provide a diagnosis using models provided from the centralised medical facility. This type of application is being updated often as more images are used to train models and expand the machine learning capabilities for a diagnostic facility. An image registry provides the images needed to deploy the various applications and is updated by pushing new images from the remote medical facility as updates become available.  A separate anonymise data application is tasked with ensuring any patient information and imagery is scrubbed before transporting to the main medical facility where it's made available for further research test sets, model training sets, and other activities.  The notification application is purely a communication vehicle for necessary information between clinical personnel, patients, and other parties as deemed necessary in the diagnostics process. Without a doubt, one of the most important aspects to any solution of this nature is the integration of information, storage, and applications which we find here in the distributed streaming services. These services ensure near real-time streaming of data, images, and communications across the diagnostic facility, with the edge devices, all manner of user interactions, and with the centralised medical facility.  Finally, the object storage element is of course used to ensure persistence of all manner of objects, such as patient data, images, diagnosis results, and much more. MEDICAL DATACENTER - CONTAINER PLATFORM Back in the centralised medical organisation we find the teams that maintain and deliver on the applications, models, dashboards, and the rest of this medical diagnosis solutions. This collection of elements contains all the base tooling that makes data, models, machine learning, and artificial intelligence applicable for maintaining and improving the medical diagnosis on the edge. Starting with the top right element in this image, the machine learning element is where models are created, tested on data sets, and tagged for use in the various applications being provided to the edge users.  A dashboard application signifies the ability to monitor the processing of image diagnostics, images, data, and how the entire organisation from edge to backend is performing over time. This can be used to tweak the deployment and usage of the medical diagnostic applications as needed. The image registry here is used to collect the newly updated and developed application images for eventual sharing out to the image registries located at the supported diagnostic facilities discussed above. As always, the object storage element is of course used to ensure persistence of all manner of objects, such as patient data, images, diagnosis results, and much more. The image upload application is used to take the edge-captured images pushed from the diagnostic center for analysis. These are also used to improve the various artificial intelligence and machine learning models applied to the detection application. They also provide historical data sets of actual images, that after being anonymised, can further many of the research projects such organisations pursue. There is a manual classification application that helps the clinical personnel with medical diagnosis where full automation was unable to bring a case to conclusion.  Finally, we find a machine learning CI/CD element that is used in the development cycle for testing and building images for deployment of applications and other tooling to the image registry. WHAT'S NEXT This was just a short overview of the common generic elements that make up our architecture for the edge medical diagnosis use case.  An overview of this series on edge medical diagnosis architecture: 1. 2. 3. Example predictive analysis 4. Example architecture with GitOps Catch up on any articles you missed by following one of the links above. Next in this series, taking a look at an example predictive analysis to provide you with a map for your own medical diagnostic solutions.</content><dc:creator>Eric D. Schabell</dc:creator></entry><entry><title type="html">Using The RESTEasy Galleon Feature Pack In WildFly</title><link rel="alternate" href="https://resteasy.github.io/2021/11/22/feature-pack/" /><author><name /></author><id>https://resteasy.github.io/2021/11/22/feature-pack/</id><updated>2021-11-22T00:00:00Z</updated><dc:creator /></entry><entry><title>Improve multicore scaling in Open vSwitch DPDK</title><link rel="alternate" href="https://developers.redhat.com/articles/2021/11/19/improve-multicore-scaling-open-vswitch-dpdk" /><author><name>Kevin Traynor</name></author><id>d4750d8d-a50c-4633-9329-b7a4e8d905ac</id><updated>2021-11-19T07:00:00Z</updated><published>2021-11-19T07:00:00Z</published><summary type="html">&lt;p&gt;A new feature in version 2.16 of &lt;a href="https://docs.openvswitch.org/"&gt;Open vSwitch (OVS)&lt;/a&gt; helps developers scale the &lt;a href="https://docs.openvswitch.org/en/latest/intro/install/dpdk/"&gt;OVS-DPDK&lt;/a&gt; userspace datapath to use multiple cores. The &lt;a href="https://www.dpdk.org"&gt;Data Plane Development Kit (DPDK)&lt;/a&gt; is a popular set of networking libraries and drivers that provide fast packet processing and I/O.&lt;/p&gt; &lt;p&gt;After reading this article, you will understand the new &lt;code&gt;group&lt;/code&gt; assignment type for spreading the datapath workload across multiple cores, how this type differs from the default &lt;code&gt;cycles&lt;/code&gt; assignment type, and how to use the new type in conjunction with the auto load balance feature in the poll mode driver (PMD) to improve OVS-DPDK scaling.&lt;/p&gt; &lt;h2 id="background"&gt;How PMD threads manage packets in the userspace datapath&lt;/h2&gt; &lt;p&gt;In the OVS-DPDK userspace datapath, receive queues (RxQs) store packets from an interface that need to be received, processed, and usually transmitted to another interface. This work is done in OVS-DPDK by PMD threads that run on a set of dedicated cores and that continually poll the RxQs for packets. In OVS-DPDK, these datapath cores are commonly referred to just as PMDs.&lt;/p&gt; &lt;p&gt;When there's more than one PMD, the workload should ideally be spread equally across them all. This prevents packet loss in cases where some PMDs may be overloaded while others have no work to do. In order to spread the workload across the PMDs, the interface RxQs that provide the packets need to be carefully assigned to the PMDs.&lt;/p&gt; &lt;p&gt;The user can manually assign individual RxQs to PMDs with the &lt;code&gt;other_config:pmd-rxq-affinity&lt;/code&gt; option. By default, OVS-DPDK also automatically assigns them. In this article, we focus on OVS-DPDK's process for automatically assigning RxQs to PMDs.&lt;/p&gt; &lt;h3 id="ovs-dpdk-automatic-assignment"&gt;OVS-DPDK automatic assignment&lt;/h3&gt; &lt;p&gt;RxQs can be automatically assigned to PMDs when there is a reconfiguration, such as the addition or removal of either RxQs or PMDs. Automatic assignment also occurs if triggered by the PMD auto load balance feature or the &lt;code&gt;ovs-appctl dpif-netdev/pmd-rxq-rebalance&lt;/code&gt; command.&lt;/p&gt; &lt;p&gt;The default &lt;code&gt;cycles&lt;/code&gt; assignment type assigns the RxQs requiring the most processing cycles to different PMDs. However, the assignment also places the same or similar number of RxQs on each PMD.&lt;/p&gt; &lt;p&gt;The &lt;code&gt;cycles&lt;/code&gt; assignment type is a trade-off between optimizing for the current workload and having the RxQs spread out across PMDs to mitigate against workload changes. The default type is designed this way because, when it was introduced in OVS 2.9, there was no PMD auto load balance feature to deal with workload changes.&lt;/p&gt; &lt;h3 id="the-role-of-pmd-auto-load-balance"&gt;The role of PMD auto load balance&lt;/h3&gt; &lt;p&gt;PMD auto load balance is an OVS-DPDK feature that dynamically detects an imbalance created by the user in how the workload is spread across PMDs. If PMD auto load balance estimates that the workload can and should be spread more evenly, it triggers an RxQ-to-PMD reassignment. The reassignment, and the ability to rebalance the workload evenly among PMDs, depends on the RxQ-to-PMD assignment type.&lt;/p&gt; &lt;p&gt;PMD auto load balance is discussed in more detail in another &lt;a href="https://developers.redhat.com/blog/2021/04/29/automatic-load-balancing-for-pmd-threads-in-open-vswitch-with-dpdk"&gt;article&lt;/a&gt;.&lt;/p&gt; &lt;h2 id="the-group-rxq-to-pmd-assignment-type"&gt;The group RxQ-to-PMD assignment type&lt;/h2&gt; &lt;p&gt;In OVS 2.16, the &lt;code&gt;cycles&lt;/code&gt; assignment type is still the default, but a more optimized &lt;code&gt;group&lt;/code&gt; assignment type was added.&lt;/p&gt; &lt;p&gt;The main differences between these assignment types is that the &lt;code&gt;group&lt;/code&gt; assignment type removes the trade-off of having similar numbers of RxQs on each PMD. Instead, this assignment type spreads the workload purely based on finding the best current balance of the workload across PMDs. This improved optimization is feasible now because the PMD auto load balance feature is available to deal with possible workload changes.&lt;/p&gt; &lt;p&gt;The &lt;code&gt;group&lt;/code&gt; assignment type also scales better, because it recomputes the estimated workload on each PMD before every RxQ assignment.&lt;/p&gt; &lt;p&gt;The increased optimization can mean a more equally distributed workload and hence more equally distributed available capacity across the PMDs. This improvement, along with PMD auto load balance, can mitigate against changes in workload caused by changes in traffic profiles.&lt;/p&gt; &lt;h3 id="an-rxq-to-pmd-assignment-comparison"&gt;An RxQ-to-PMD assignment comparison&lt;/h3&gt; &lt;p&gt;We can see some of the differing characteristics of &lt;code&gt;cycles&lt;/code&gt; and &lt;code&gt;group&lt;/code&gt; with an example. If we run OVS 2.16 with a couple RxQs and PMDs, we can check the log messages to confirm that the default &lt;code&gt;cycles&lt;/code&gt; assignment type is used for assigning Rxqs to PMDs:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;|dpif_netdev|INFO|Performing pmd to rx queue assignment using cycles algorithm.&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Then we can take a look at the current RxQ PMD assignments and RxQ workload usage:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ ovs-appctl dpif-netdev/pmd-rxq-show&lt;/code&gt;&lt;/pre&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;pmd thread numa_id 0 core_id 8: isolated : false port: vhost1 queue-id: 0 (enabled) pmd usage: 20 % port: dpdk0 queue-id: 0 (enabled) pmd usage: 70 % overhead: 0 % pmd thread numa_id 0 core_id 10: isolated : false port: vhost0 queue-id: 0 (enabled) pmd usage: 20 % port: dpdk1 queue-id: 0 (enabled) pmd usage: 30 % overhead: 0 %&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The workload is visualized in Figure 1.&lt;/p&gt; &lt;figure class="align-center rhd-u-has-filter-caption" role="group"&gt;&lt;img alt="cycles RxQ-to-PMD assignment" data-entity-type="file" data-entity-uuid="e05e1ca4-49b2-45aa-9cee-c21108884027" src="https://developers.redhat.com/sites/default/files/inline-images/cycles.jpg" /&gt;&lt;figcaption class="rhd-c-caption"&gt;Figure 1. cycles RxQ-to-PMD assignment.&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;The display shows that the &lt;code&gt;cycles&lt;/code&gt; assignment type has done a good job keeping the two RxQs that require the most cycles (&lt;code&gt;dpdk0&lt;/code&gt; 70% and &lt;code&gt;dpdk1&lt;/code&gt; 30%) on different PMDs. Otherwise, one PMD would be at 100% and Rx packets might be dropped as a result.&lt;/p&gt; &lt;p&gt;The display also shows that the assignment insists on both PMDs having an equal number of RxQs, two each. This means that PMD 8 is 90% loaded while PMD 10 is 50% loaded.&lt;/p&gt; &lt;p&gt;That is not a problem with the current traffic profile, because both PMDs have enough processing cycles to handle the load. However, it does mean that PMD 8 has available capacity of only 10% to account for any traffic profile changes that require more processing. If, for example, the &lt;code&gt;dpdk0&lt;/code&gt; traffic profile changed and the required workload increased by more than 10%, PMD 8 would be overloaded and packets would be dropped.&lt;/p&gt; &lt;p&gt;Now we can look at how the &lt;code&gt;group&lt;/code&gt; assignment type optimizes for this kind of scenario. First we enable the &lt;code&gt;group&lt;/code&gt; assignment type:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ ovs-vsctl set Open_vSwitch . other_config:pmd-rxq-assign=group&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The logs confirm that is selected and immediately put to use:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;|dpif_netdev|INFO|Rxq to PMD assignment mode changed to: `group`.&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;As mentioned earlier, the &lt;code&gt;group&lt;/code&gt; assignment type eliminates the requirement of keeping the same number of RxQs per PMD, and bases its assignments on estimates of the least loaded PMD before every RxQ assignment. We can see how this policy affects the assignments:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ ovs-appctl dpif-netdev/pmd-rxq-show&lt;/code&gt;&lt;/pre&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;pmd thread numa_id 0 core_id 8: isolated : false port: dpdk0 queue-id: 0 (enabled) pmd usage: 70 % overhead: 0 % pmd thread numa_id 0 core_id 10: isolated : false port: vhost0 queue-id: 0 (enabled) pmd usage: 20 % port: dpdk1 queue-id: 0 (enabled) pmd usage: 30 % port: vhost1 queue-id: 0 (enabled) pmd usage: 20 % overhead: 0 %&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The workload is visualized in Figure 2.&lt;/p&gt; &lt;figure class="align-center rhd-u-has-filter-caption" role="group"&gt;&lt;img alt="group RxQ-to-PMD assignment type" data-entity-type="file" data-entity-uuid="62c09dcf-5452-4a90-be7e-df9bc0d1e139" src="https://developers.redhat.com/sites/default/files/inline-images/group.jpg" /&gt;&lt;figcaption class="rhd-c-caption"&gt;Figure 2. group RxQ-to-PMD assignment type.&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;Now PMD 8 and PMD 10 both have total loads of 70%, so the workload is better balanced between the PMDs.&lt;/p&gt; &lt;p&gt;In this case, if the &lt;code&gt;dpdk0&lt;/code&gt; traffic profile changes and the required workload increases by 10%, it could be handled by PMD 8 without any packet drops because there is 30% available capacity.&lt;/p&gt; &lt;p&gt;An interesting case is where RxQs are new or have no measured workload. If they were all put on the least loaded PMD, that PMD's estimated workload would not change. It would keep being selected as the least loaded PMD and be assigned all the new RxQs. This might not be ideal if those RxQs later became active, so instead the &lt;code&gt;group&lt;/code&gt; assignment type spread RxQs with no measured history among PMDs.&lt;/p&gt; &lt;p&gt;This example shows a change from the &lt;code&gt;cycles&lt;/code&gt; to &lt;code&gt;group&lt;/code&gt; assignment type during operation. Although that can be done,  the assignment type is typically set when initializing OVS-DPDK. Reassignments can then be triggered by any of the following mechanisms:&lt;/p&gt; &lt;ul&gt;&lt;li&gt;PMD auto load balance (providing that user-defined thresholds are met)&lt;/li&gt; &lt;li&gt;A change in configuration (adding or removing RxQs or PMDs)&lt;/li&gt; &lt;li&gt;The &lt;code&gt;ovs-appctl dpif-netdev/pmd-rxq-rebalance&lt;/code&gt; command&lt;/li&gt; &lt;/ul&gt;&lt;h2 id="other-rxq-considerations"&gt;Other RxQ considerations&lt;/h2&gt; &lt;p&gt;All of the OVS-DPDK assignment types are constrained by the granularity of the workload on each RxQ. In the example in the previous section, it was possible to spread the workload evenly. In a case where &lt;code&gt;dpdk0&lt;/code&gt; was 95% loaded instead, PMD 8 would have a 95% load, while PMD 10 would have a 70% load.&lt;/p&gt; &lt;p&gt;If you expect an interface to have a high traffic rate and hence a high required load, it is worth considering the addition of more RxQs in order to help split the traffic for that interface. More RxQs mean a greater granularity to help OVS-DPDK spread the workload more evenly across the PMDs.&lt;/p&gt; &lt;h2 id="conclusion"&gt;Conclusion&lt;/h2&gt; &lt;p&gt;This article looked at the new &lt;code&gt;group&lt;/code&gt; assignment type from OVS 2.16 for RxQ-to-PMD assignments.&lt;/p&gt; &lt;p&gt;Although the existing &lt;code&gt;cycles&lt;/code&gt; assignment type might be good enough in many cases, the new &lt;code&gt;group&lt;/code&gt; assignment type allows OVS-DPDK to more evenly distribute the workload across the available PMDs.&lt;/p&gt; &lt;p&gt;This dynamic assignment has the benefit of allowing more optimal use of PMDs and providing a more equally distributed available capacity across PMDs, which in turn can make them more resilient against workload changes. For larger changes in workload, the PMD auto load balance feature can trigger reassignments.&lt;/p&gt; &lt;p&gt;OVS 2.16 still has the same defaults as OVS 2.15, so users for whom OVS 2.15 multicore scaling is good enough can continue to use it by default after an upgrade. However, the new option is available if required.&lt;/p&gt; &lt;p&gt;Further information about OVS-DPDK PMDs can be found in the &lt;a href="https://docs.openvswitch.org/en/latest/topics/dpdk/pmd/"&gt;documentation&lt;/a&gt;.&lt;/p&gt; The post &lt;a href="https://developers.redhat.com/articles/2021/11/19/improve-multicore-scaling-open-vswitch-dpdk" title="Improve multicore scaling in Open vSwitch DPDK"&gt;Improve multicore scaling in Open vSwitch DPDK&lt;/a&gt; appeared first on &lt;a href="https://developers.redhat.com/blog" title="Red Hat Developer"&gt;Red Hat Developer&lt;/a&gt;. &lt;br /&gt;&lt;br /&gt;</summary><dc:creator>Kevin Traynor</dc:creator><dc:date>2021-11-19T07:00:00Z</dc:date></entry><entry><title type="html">Kogito Tooling Released! 10k+ installs on BPMN extension, Dashbuilder Runtime in Quarkus, and an outstanding KIE Live next week!</title><link rel="alternate" href="https://blog.kie.org/2021/11/kogito-tooling-released-10k-installs-on-bpmn-extension-dashbuilder-runtime-in-quarkus-and-an-outstanding-kie-live-next-week.html" /><author><name>Eder Ignatowicz</name></author><id>https://blog.kie.org/2021/11/kogito-tooling-released-10k-installs-on-bpmn-extension-dashbuilder-runtime-in-quarkus-and-an-outstanding-kie-live-next-week.html</id><updated>2021-11-19T05:00:00Z</updated><content type="html">We have just launched a fresh new Kogito Tooling release! &#x1f389; On the 0.14.1 , we made a lot of improvements and bug fixes. This post will give a quick overview of our most recent . I hope you enjoy it! DON’T MISS THE KIE LIVE NEXT WEEK Our beloved .new environment will receive a massive update! Dealing with complex models and collaborating with others will become much easier. Join us on the 23rd of November in the next for a walkthrough of the new features and new integrations we have with the Developers’ most beloved tools. 10K+ USERS OF VS CODE BPMN EXTENSION Our just reached an important milestone on VS Code Store: 10k+ individual installs! Congrats to Roger (tech lead) and all the BPMN/Stunner team. AUTOMATICALLY GENERATE BPMN/DMN SVG ON VS CODE To provide better integration with the KIE server and Business Central, on the Kogito Tooling 0.14 release, we introduced a way to, on VS Code, automatically generate SVG on each save of your BPMN and DMN Diagram. Take a look at this feature in action: Please take a look at this for further details! DASHBUILDER RUNTIME RELEASED ON KOGITO TOOLING We are glad to announce that we are releasing ! The major change for this new release is the adoption of Quarkus as the backend for DashBuilder Runtime and the introduction of DashBuilder Authoring, a new tool to create dashboards. From now on, we will also follow the Kogito Tooling release cadence! Soon we will publish a blog post with more details! CANVAS API We also just released the first iteration of a for node manipulation in our editors. This new API allows to manipulate the shapes in the canvas, so third parties can play with the different objects in the canvas VISUALIZE, EDIT, AND SHARE YOUR BPMN, DMN, AND PMML WITH GITHUB.DEV Some weeks ago, GitHub released github.dev which allows you to open any repository in VS Code directly from your browser just pressing . (dot key) on it. On Kogito Tooling 0.13.0 release, we updated our VS Code BPMN, DMN, and PMML extension to also work on this innovative environment. Check it out: Please take a look at this for further details! NEW FEATURES, FIXED ISSUES, AND IMPROVEMENTS * – Editor content sanitization * – [DMN Designer] User changes are lost * – Generate a SVG diagram automatically on each BPMN/DMN diagrams save * – [DMN Designer] Improve BKM description rendering on documentation tab * – Stunner – Create an initial JS / TS API for accessing the canvas and its elements * – ScoreCard: MiningField validation * – Mining Schema (PMML Editor test Suite) * – [VSCode] Custom editor save issues * – Editors – Editing the node name and pressing enter to confirm * – [DMN Designer] Unreadable data type information in PDF document that shows DMN decision model * – [DMN Designer] Background color do not work on DMN Editor (online and VSCode) * – [DMN Designer] Multiple DRDs – Renaming a DRD freezes the browser * – Online DMN Editor should support deployment to any Openshift Cluster other than Dev Sandbox * – Collections Data Objects can be filled with expressions only. * – Standalone DMN editor missing isDirty indication on data type or included models change * – Create the second step of the Wizard – Create the collapsible/expandable list of future Data Types * – BPMN Editor – Containment not working when Node overlaps the Connector while splicing * – formInputs should be parsed with dates as objects, not strings. * – Get the Route through the REST API and remove the console URL property. * – Enable extensions for github.dev. * – Activate the DMN dirty indicator test * – DMN Guided tour cypress tests * – Introduce DMN Runner cypress test * – ScoreCard Model Setup Test * – Score Cards: Algorithm Name cannot be cleared * – Score Cards: Data Dictionary: Remove duplication of delete icons * – Metadata atrribute ‘elementname’ not present for events, intermediate events &amp;amp; gateways by default * – Filled DateTime field on Dev Sandbox form break Runner when it’s opened on Online Editor * – kogito-examples non unique packages * – kogito-editors-java pre push hooks * – [DMN Designer] New Boxed Expression editor – Remove grip from the new boxed expression editor * – Improvements for the KIE Tooling Extended Services outdated icon * – Fix Quarkus Dev UI DEV mode FURTHER READING/WATCHING We had some excellent blog posts on Kie Blog and Kie Lives that I recommend to you: * , by Paulo; * , by Tiago Dolphine; * , by Pere; * , by Paulo; THANK YOU TO EVERYONE INVOLVED! I would like to thank everyone involved with this release, from the excellent KIE Tooling Engineers to the lifesavers QEs and the UX people that help us look awesome! The post appeared first on .</content><dc:creator>Eder Ignatowicz</dc:creator></entry><entry><title>Design an authorization cache for Envoy proxy using WebAssembly</title><link rel="alternate" href="https://developers.redhat.com/articles/2021/11/18/design-authorization-cache-envoy-proxy-using-webassembly" /><author><name>Rahul Anand</name></author><id>00c1e42d-6885-4d0a-9491-1dd126cf8007</id><updated>2021-11-18T07:00:00Z</updated><published>2021-11-18T07:00:00Z</published><summary type="html">&lt;p&gt;This article introduces a high-level design to implement an authorization cache associated with the Envoy proxy using WebAssembly. The goal of this project is to reduce the latencies of HTTP requests passing through the Envoy proxy by reducing the traffic to the service responsible for authentication and authorization of requests. The cache stores data about authorization so that the external service needs to be contacted only on cache misses, instead of for every HTTP request.&lt;/p&gt; &lt;p&gt;We also provide the source code of an authorization cache that interacts with &lt;a href="https://developers.redhat.com/products/3scale/overview"&gt;Red Hat 3scale API Management&lt;/a&gt;. The cache was implemented as a part of the &lt;a href="https://summerofcode.withgoogle.com/archive/2021/projects/6551776420954112/"&gt;Google Summer of Code 2021 project&lt;/a&gt;. &lt;/p&gt; &lt;p&gt;This article is the first in a two-part series. This first article introduces a high-level, generic design that will give you a basic idea of the cache's overall functionality. The second part explains the major design decisions and implementation details.&lt;/p&gt; &lt;h2&gt;What is Envoy proxy?&lt;/h2&gt; &lt;p&gt;&lt;a href="https://www.envoyproxy.io/"&gt;Envoy&lt;/a&gt; is an &lt;a href="https://developers.redhat.com/topics/open-source"&gt;open source&lt;/a&gt; &lt;a href="https://developers.redhat.com/topics/edge-computing"&gt;edge&lt;/a&gt; and service proxy for applications running in the cloud. Envoy is valuable for many use cases, including edge proxy, middle proxy, sidecar for service mesh deployments, and a daemon set within &lt;a href="https://developers.redhat.com/topics/kubernetes/"&gt;Kubernetes&lt;/a&gt;. Among Envoy's compelling features, performance, extensibility, and &lt;a href="https://developers.redhat.com/topics/api-management/"&gt;API configurability&lt;/a&gt; are the most prominent, making it unique in the proxy space. This article mainly focuses on extensibility. &lt;/p&gt; &lt;h2&gt;The Envoy proxy authorization cache in action&lt;/h2&gt; &lt;p&gt;The implementation uses &lt;a href="https://github.com/proxy-wasm/"&gt;Proxy-Wasm&lt;/a&gt;, implementing the extensions as &lt;a&gt;WebAssembly&lt;/a&gt; modules. We made this choice based on the flexibility, portability, maintainability, and isolation (for fault tolerance) that WebAssembly offers when compared with native Envoy filters. At the time of writing this article, Proxy-Wasm supports two types of extensions: Filters and singleton services.&lt;/p&gt; &lt;p&gt;Figure 1 shows the interactions between the proxy, the 3scale Service Management API, and the upstream services when a client sends a request. The request is propagated through the Envoy filter chain. For access to Wasm filters and services, the request interacts with the Wasm Virtual machine (VM) to execute the Wasm plugins.&lt;/p&gt; &lt;figure class="align-center rhd-u-has-filter-caption" role="group"&gt;&lt;img alt="Higher Level Introductory Diagram" data-entity-type="file" data-entity-uuid="1a15c798-b5ea-40d4-a666-1a11dcdf6969" src="https://developers.redhat.com/sites/default/files/inline-images/Blog%20Diagrams%20-%20Higher%20Level%20Intro_0.png" /&gt;&lt;figcaption class="rhd-c-caption"&gt;Figure1. Envoy communicates through the Wasm virtual machines with the 3scale Service Management API.&lt;/figcaption&gt;&lt;/figure&gt;&lt;h2&gt;Designing the authorization cache&lt;/h2&gt; &lt;p&gt;In our design, the cache inside the proxy is implemented through two main components: A filter and a singleton service.&lt;/p&gt; &lt;p&gt;The filter is responsible for intercepting HTTP requests, authorizing them based on the stored cache, and performing rate limiting. In the context of the envoy, this component is an HTTP filter and gets executed in the worker threads. For each request, a context object gets created.&lt;/p&gt; &lt;p&gt;The singleton service is responsible for the background synchronization of cached data between the proxy and the 3scale Service Management API. In the context of the envoy, this is a singleton service and gets executed in the main thread outside the request lifecycle. Only one instance of this service gets instantiated in each Envoy process.&lt;/p&gt; &lt;p&gt;Figure 2 shows how the HTTP filter and singleton service interact with the other internal and external components to provide the expected functionality of the in-proxy cache. Shared data is an in-memory key-value store specified by the Proxy-Wasm ABI and provided by the proxy. Each VM contains a shared datastore. Because the two extensions (filter and singleton) are running in the same Wasm VM, both extensions have direct access to the shared data.&lt;/p&gt; &lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content-full-width"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/overall.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_full_width_1440px_w/public/overall.png?itok=zqS9QtQJ" width="1440" height="706" alt="The filter's threads and the singleton service share data and a message queue." typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 2. The filter's threads and the singleton service share data and a message queue. &lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;p&gt;I/O between the host and VM is done in binary format and thus requires serialization and deserialization. The Proxy-Wasm ABI provides a shared queue that is also unique per VM. In our design, the shared queue is the main communication channel between the singleton service and filter. Any message enqueued is broadcast to all threads, but only one thread can dequeue the message. The singleton service updates the cache records saved in the shared data, either periodically or based on policies defined in the configuration. The HTTP filter uses the cache records in the shared data to perform authorization and rate-limiting.&lt;/p&gt; &lt;h3&gt;Filter design&lt;/h3&gt; &lt;p&gt;The default Envoy proxy makes an external HTTP call to the 3scale Service Management API for every HTTP request to perform authorization and reporting. But with the internal cache, we eliminate this need and limit the external HTTP calls to cache misses (Figure 3). That way, we reduce the traffic on the 3scale Service Management API and therefore overall request latency, to a great extent.&lt;/p&gt; &lt;figure class="align-center rhd-u-has-filter-caption" role="group"&gt;&lt;img alt="Envoy filter chain with and without cache filter" data-entity-type="file" data-entity-uuid="ecac817d-8571-4ca0-95d6-6012b8f44463" src="https://developers.redhat.com/sites/default/files/inline-images/Blog%20Diagrams%20-%20W%20and%20W_O%20cache_0.png" /&gt;&lt;figcaption class="rhd-c-caption"&gt;Figure 3. Envoy proxy filter chain with and without cache filter.&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;The filter serves as the entry point for the request authorization with the help of cached data. The filter takes one of two pathways for each request:&lt;/p&gt; &lt;ul&gt;&lt;li&gt;&lt;strong&gt;Cache miss:&lt;/strong&gt; This happens when a cache record is not found in the shared data. The filter calls out to the 3scale Service Management API to fetch the latest state.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Cache hit:&lt;/strong&gt; This happens when a cache record is present. Based on authorization results, metrics are passed to the singleton for reporting to the 3scale Service Management API.&lt;/li&gt; &lt;/ul&gt;&lt;p&gt;The filter is equipped with an opt-in feature called &lt;em&gt;unique-callout&lt;/em&gt;, which ensures that there is only one callout to the 3scale Service Management API on a cache miss. This feature increases performance and accuracy under high load, without any impact under low-load conditions. We'll look at the results with and without the unique callout feature in the second part of this series, when we get to examine the benchmarks.&lt;/p&gt; &lt;h3&gt;Singleton design&lt;/h3&gt; &lt;p&gt;The singleton service serves two main functions for the proposed in-proxy cache design:&lt;/p&gt; &lt;ul&gt;&lt;li&gt;Collect metrics based on a predefined policy and report them to the external management service.&lt;/li&gt; &lt;li&gt;Update the local cache stored in the proxy by pulling the latest states of applications and services from the external management service.&lt;/li&gt; &lt;/ul&gt;&lt;p&gt;In our case, the external management service is the 3scale Service Management API.&lt;/p&gt; &lt;p&gt;In Envoy, almost all the functions in worker threads and the main thread get executed in a non-blocking manner. So the methods get invoked as callbacks. In the singleton service, we use two types of events to perform the required functionality:&lt;/p&gt; &lt;ul&gt;&lt;li&gt;Periodic events, triggered at predefined intervals via the &lt;code&gt;on_tick()&lt;/code&gt; callback&lt;/li&gt; &lt;li&gt;Events triggered by the filter through the message queue&lt;/li&gt; &lt;/ul&gt;&lt;p&gt;Events that are sent from the filter pass through the message queue to the singleton service. The second part of this article offers further details about the integration of cache with the singleton section.&lt;/p&gt; &lt;h2&gt;Coming up: Implementing the cache&lt;/h2&gt; &lt;p&gt;In the next part of this series, we'll dive into the implementation details of the cache, data modeling, various features that made the cache work better, limitations, and planned improvements. &lt;/p&gt; &lt;p&gt;In the meantime, here are additional resources to learn more:&lt;/p&gt; &lt;ul&gt;&lt;li&gt;To try this module, check out the &lt;a href="https://github.com/rahulanand16nov/gsoc-wasm-filters/"&gt;project repository on GitHub&lt;/a&gt;.&lt;/li&gt; &lt;li&gt;For a demo using the module integrated with Istio, check out this session presented by Daniel Grimm and Burr Sutter: &lt;a href="https://developers.redhat.com/devnation/tech-talks/istio-webassembly"&gt;Hacking the Mesh: Extending Istio with WebAssembly Modules | DevNation Tech Talk&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; The post &lt;a href="https://developers.redhat.com/articles/2021/11/18/design-authorization-cache-envoy-proxy-using-webassembly" title="Design an authorization cache for Envoy proxy using WebAssembly"&gt;Design an authorization cache for Envoy proxy using WebAssembly&lt;/a&gt; appeared first on &lt;a href="https://developers.redhat.com/blog" title="Red Hat Developer"&gt;Red Hat Developer&lt;/a&gt;. &lt;br /&gt;&lt;br /&gt;</summary><dc:creator>Rahul Anand</dc:creator><dc:date>2021-11-18T07:00:00Z</dc:date></entry><entry><title type="html">Edge medical diagnosis - Architectural introduction</title><link rel="alternate" href="http://www.schabell.org/2021/11/edge-medical-diagnosis-architectural-introduction.html" /><author><name>Eric D. Schabell</name></author><id>http://www.schabell.org/2021/11/edge-medical-diagnosis-architectural-introduction.html</id><updated>2021-11-18T06:00:00Z</updated><content type="html">Part 1 - Architectural introduction The last few years we have been digging deeply into the world of architectures with a focus on presenting access to ways of mapping successful implementations for specific use cases. It's an interesting challenge in that we have the mission of creating architectural content based on common customer adoption patterns.  That's very different from most of the traditional marketing activities usually associated with generating content for the sole purpose of positioning products for solutions. When you're basing the content on actual execution in solution delivery, you're cutting out the chuff.  What's that mean? It means that it's going to provide you with a way to implement a solution using open source technologies by focusing on the integrations, structures and interactions that actually have been proven to work. What's not included are any vendor promises that you'll find in normal marketing content. Those promised that when it gets down to implementation crunch time, might not fully deliver on their promises. Enter the term Portfolio Architecture.  Let's look at these architectures, how they're created and what value they provide for your solution designs. THE PROCESS The first step is to decide the use case to start with, which in my case had to be linked to a higher level theme that becomes the leading focus. This higher level theme is not quite boiling the ocean, but it's so broad that it's going to require some division into smaller parts. In this case presented here is we are looking closer at the healthcare industry and we've decided to start with one we're calling the edge medical diagnosis architecture. This use case we've defined as the following: Accelerating medical diagnosis using condition detection in medical imagery with AI/ML at medical facilities  The approach taken is to research our existing customers that have implemented solutions in this space, collect their public-facing content, research the internal implementation documentation collections from their successful engagements, and where necessary reach out to the field resources involved.  To get an idea of what these architectures look like, we refer you to the series previously discussed here: * * * * Now on to the task at hand. WHAT'S NEXT The resulting content for this project targets the following three items. * A slide deck of the architecture for use in telling the portfolio solution story. * Generic architectural diagrams providing the general details for the portfolio solution. * A write-up of the portfolio solution in a series that can be used for a customer solution brief. An overview of this series on edge medical diagnosis architecture: 1. 2. 3. Example predictive analysis 4. Example architecture with GitOps Catch up on any past articles you missed by following any published links above. Next in this series, we will take a look at the generic  for the edge medical diagnosis architecture.</content><dc:creator>Eric D. Schabell</dc:creator></entry><entry><title type="html">This Week in JBoss - 18 November 2021</title><link rel="alternate" href="https://www.jboss.org/posts/weekly-2021-11-18.html" /><category term="quarkus" /><category term="kogito" /><category term="java" /><category term="resteasy" /><category term="camel" /><category term="reactive" /><author><name>Alex Porcelli</name><uri>https://www.jboss.org/people/alex-porcelli</uri><email>do-not-reply@jboss.com</email></author><id>https://www.jboss.org/posts/weekly-2021-11-18.html</id><updated>2021-11-18T00:00:00Z</updated><content type="html">&lt;article class="" data-tags="quarkus, kogito, java, resteasy, camel, reactive"&gt; &lt;h1&gt;This Week in JBoss - 18 November 2021&lt;/h1&gt; &lt;p class="preamble"&gt;&lt;/p&gt;&lt;p&gt;Hello! Welcome to another edition of the JBoss Editorial that brings you news and updates from our community.&lt;/p&gt;&lt;p&gt;&lt;/p&gt; &lt;div class="sect1"&gt; &lt;h2 id="_release_roundup"&gt;Release roundup&lt;/h2&gt; &lt;div class="sectionbody"&gt; &lt;p&gt;Here are the releases from the JBoss Community for this edition:&lt;/p&gt; &lt;div class="ulist square"&gt; &lt;ul class="square"&gt; &lt;li&gt; &lt;p&gt;&lt;a href="https://quarkus.io/blog/quarkus-2-4-2-final-released/"&gt;Quarkus 2.4.2&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;&lt;a href="https://blog.kie.org/2021/11/kogito-1-13-0-released.html"&gt;Kogito 1.13.0&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;&lt;a href="https://camel.apache.org/blog/2021/11/RELEASE-3.13.0/"&gt;Camel 3.13.0&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;&lt;a href="https://camel.apache.org/blog/2021/11/camel-quarkus-release-2.4.0/"&gt;Camel Quarkus 2.4.0&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;&lt;a href="https://github.com/apache/camel-k/releases/tag/v1.7.0/"&gt;Camel K 1.7.0&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;&lt;a href="https://in.relation.to/2021/11/09/hibernate-reactive-1_1_0_Final/"&gt;Hibernate Reactive 1.1.0.Final&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;&lt;a href="https://resteasy.github.io/2021/11/04/resteasy-5.0.0-release/"&gt;RESTEasy 5.0.0&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &lt;/ul&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class="sect1"&gt; &lt;h2 id="_test_driven_development_with_quarkus"&gt;Test-driven development with Quarkus&lt;/h2&gt; &lt;div class="sectionbody"&gt; &lt;p&gt;&lt;a href="https://developers.redhat.com/articles/2021/11/08/test-driven-development-quarkus"&gt;Test-driven development with Quarkus&lt;/a&gt;, by Eric Deandrea&lt;/p&gt; &lt;p&gt;Eric’s article provides a detailed walkthrough of how to take advantage of Quarkus continuous testing and Dev UI for test-driven development. Eric also provides a sample repository that you can use to follow along with the article.&lt;/p&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class="sect1"&gt; &lt;h2 id="_boost_throughput_with_resteasy_reactive_in_quarkus_2"&gt;Boost throughput with RESTEasy Reactive in Quarkus 2.&lt;/h2&gt; &lt;div class="sectionbody"&gt; &lt;p&gt;&lt;a href="https://developers.redhat.com/articles/2021/11/04/boost-throughput-resteasy-reactive-quarkus-22/"&gt;Boost throughput with RESTEasy Reactive in Quarkus 2.&lt;/a&gt;, by Daniel Oh&lt;/p&gt; &lt;p&gt;Daniel shows how some simple changes in your REST endpoints can boost the throughput of your application using RESTEasy Reactive. He also shows how to use the Endpoint score dashboard to assess the performance of your endpoints.&lt;/p&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class="sect1"&gt; &lt;h2 id="_low_code_camel"&gt;Low Code Camel&lt;/h2&gt; &lt;div class="sectionbody"&gt; &lt;p&gt;&lt;a href="https://www.nicolaferraro.me/2021/11/03/low-code-camel/]"&gt;Low Code Camel - How Kamelets enable a low code integration experience.&lt;/a&gt;, by Nicola Ferraro&lt;/p&gt; &lt;p&gt;This excellent post shows how Kamelets is driving a deeper transformation towards "low code" development with Camel. Nicola Ferraro shows that you can use Kamelets directly with yaml or take advantage of the Karavan tool to design and visualize your integration flows graphically.&lt;/p&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class="sect1"&gt; &lt;h2 id="_getting_started_with_hibernate_reactive_on_quarkus"&gt;Getting started with Hibernate reactive on Quarkus&lt;/h2&gt; &lt;div class="sectionbody"&gt; &lt;p&gt;&lt;a href="http://www.mastertheboss.com/soa-cloud/quarkus/getting-started-with-hibernate-reactive/"&gt;Getting started with Hibernate reactive on Quarkus&lt;/a&gt;, by F. Marchioni&lt;/p&gt; &lt;p&gt;Reactive everywhere! In this post, F. Marchioni helps us get started with Hibernate reactive, providing a step-by-step from project creation to configuration and coding.&lt;/p&gt; &lt;p&gt;The post covers the "classic" Hibernate ORM, but Hibernate Reactive can also be applied to Panache.&lt;/p&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class="sect1"&gt; &lt;h2 id="_java_lts_perspective_of_a_library_author"&gt;Java LTS - perspective of a library author&lt;/h2&gt; &lt;div class="sectionbody"&gt; &lt;p&gt;&lt;a href="https://emmanuelbernard.com/blog/2021/11/15/java-lts/"&gt;Java LTS - perspective of a library author&lt;/a&gt;, by Emmanuel Bernard&lt;/p&gt; &lt;p&gt;Oracle is proposing a change to the Java LTS lifecycle from the current 3 years for 2 years. In this post, Emmanuel Bernard put some light on the challenges, from the perspective of a library author, that such changes may bring to the complex Java ecosystem.&lt;/p&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class="sect1"&gt; &lt;h2 id="_videos"&gt;Videos&lt;/h2&gt; &lt;div class="sectionbody"&gt; &lt;p&gt;YouTube is such a great platform to share knowledge, here are my top picks for this week’s editorial:&lt;/p&gt; &lt;div class="ulist"&gt; &lt;ul&gt; &lt;li&gt; &lt;p&gt;&lt;a href="https://youtu.be/stH_jA5f5eM"&gt;First Look at Testcontainers Cloud and Quarkus&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;&lt;a href="https://youtu.be/VGAnVX1lCxg"&gt;Quarkus Insights #69: Performance and costs of reactive libraries in Java&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;&lt;a href="https://youtu.be/C5NGczQMHu0"&gt;KIELive #51 TrustyAI: Ensuring the Fairness and Transparency of Decision Models&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &lt;/ul&gt; &lt;/div&gt; &lt;p&gt;&lt;em&gt;That’s all for today! Please join us again in two weeks for another round of our JBoss editorial!&lt;/em&gt;&lt;/p&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class="author"&gt; &lt;pfe-avatar pfe-shape="circle" pfe-pattern="squares" pfe-src="/img/people/alex-porcelli.png"&gt;&lt;/pfe-avatar&gt; &lt;span&gt;Alex Porcelli&lt;/span&gt; &lt;/div&gt;&lt;/article&gt;</content><dc:creator>Alex Porcelli</dc:creator></entry></feed>
